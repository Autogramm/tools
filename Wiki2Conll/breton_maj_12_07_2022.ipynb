{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3c9d42-9598-474b-9361-229cac1d722d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# exporting the Breton example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef706cf-6a08-4a3d-8d2e-2c03bbe040c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, tqdm, re, time\n",
    "#!pip install mwclient\n",
    "#!pip install mwparserfromhell\n",
    "#!pip install mwcomposerfromhell\n",
    "from mwclient import Site\n",
    "# import dominate, pypandoc\n",
    "# from dominate.tags import *\n",
    "# from dominate.util import raw\n",
    "#myparserfromhell : parser pour le code wiki\n",
    "import mwparserfromhell, mwcomposerfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04865f40-0ac3-4c8b-91d5-69c6d23a17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ipykernel\n",
    "#!pip install mwcomposerfromhell\n",
    "#%pip install pypandoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ada57-0f99-4c23-99a6-516b1ac98aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Si on veut charger toutes les pages du site :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ae8f1-c7a4-4b77-9215-e6b7de521237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('connecting...')\n",
    "#site contenant tous les exemples\n",
    "site = Site('arbres.iker.cnrs.fr/', path='/')\n",
    "#site.login('Kimgerdes', 'azerAZER1')\n",
    "outf = open('problemPages.html','w')\n",
    "\n",
    "# images = str(list(site.allimages()))\n",
    "#print(images)\n",
    "#print(dir(images[0]))\n",
    "#print(images[0].name)\n",
    "#qsdf\n",
    "allpages = list(site.allpages())\n",
    "print('got all', len(allpages), 'pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade732-e97a-46c8-9a4c-b6448fbc080f",
   "metadata": {},
   "source": [
    "#### Si on veut sauvegarder les pages chargées dans un fichier Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1c7f7-606b-4cff-a02e-2c0f39ffbd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "#dictionnaire contenant toutes les pages aspirées\n",
    "#pages = {}\n",
    "#on peut ici modifier le nombre de pages à charger pour charger petit à petit ex : allpages[:3000] pour les 3000 premières pages\n",
    "#newlist=allpages[:8068]\n",
    "#for p in tqdm.tqdm(allpages):\n",
    "#for p in tqdm.tqdm(newlist):\n",
    "#\t#si les pages ne sont pas de la documentation\n",
    "#\tif not('/documentation' in p.name or '/docname' in p.name or 'Module:' in p.name):\n",
    "#\t\t#on ajoute le nom de la page comme clé, son contenu comme valeur\n",
    "#\t\tpages[p.name] = p.text() \n",
    "#print('extraction done. it took', time.time()-t, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b64fe-779e-446a-a721-460784bf0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#une fois toutes les pages chargées, on le met dans un pickle\n",
    "#with open(\"Dict.txt\", \"wb\") as myFile:\n",
    "#    pickle.dump(pages, myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aecd28ae-7f11-4b5a-95d1-980e7613b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on utilise le(s) pickle(s)\n",
    "import pickle\n",
    "with open(\"Dict.txt\", \"rb\") as myFile:\n",
    "    pages = pickle.load(myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76a06d9-1a49-4454-a663-84b922217f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sources.txt\", \"rb\") as file:\n",
    "    sources = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e4e3d3-31f7-4797-aa72-cedeee7497eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 8068 pages\n"
     ]
    }
   ],
   "source": [
    "# print(', '.join(pages.keys()))\n",
    "#on créé un fichier txt contenant tous les titres de pages\n",
    "open('pageTitles.txt','w', encoding=\"utf-8\").write('\\n'.join(pages.keys()))\n",
    "print('extracted',len(pages),'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d34c8233-eb54-4ed8-a66b-fbe832350c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 13422 examples\n"
     ]
    }
   ],
   "source": [
    "nbpretty=0\n",
    "#on parcourt les clés et valeurs du dictionnaire pages\n",
    "for name,text in pages.items():\n",
    "\t#et on compte le nombre de tableaux (donc d'exemples)\n",
    "\tnbpretty += text.count('class=\"prettytable\"')\n",
    "print('found',nbpretty,'examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a631ae25-f284-4a5a-b5c8-9073faafe14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "\n",
      "1\n",
      "t:Ha, tr:[[&|et]]_[[mercxxxh|femme]].[[-ed_(PL.)|s]]\n",
      "t:Ha, tr:[[&|et]]\n",
      "t:Ha, tr:[[#ed_(PL)|s]]\n",
      "1\n",
      "t:mercxxxhed, tr:[[nom_propre|Tréboul]]\n",
      "1\n",
      "t:Treboull, tr:[[kaout|avait]]\n",
      "1\n",
      "t:noe, tr:[[ket|pas]]\n",
      "1\n",
      "t:ket, tr:[[tre|tt.à.fait]]\n",
      "1\n",
      "t:tre, tr:[[art|le]]\n",
      "1\n",
      "t:ar, tr:même\n",
      "1\n",
      "t:%memes%, tr:[[taol_(M.)|coup]]_[[dorn|main]]\n",
      "1\n",
      "t:taol, tr:[[da|pour]]_[[1]]_\n",
      "yes2\n",
      "1\n",
      "t:dorn, tr:[[lakaat|mettre]]\n",
      "1\n",
      "t:da, tr:[[POSS|leur]]_[[2]]_\n",
      "yes2\n",
      "1\n",
      "t:lakaat, tr:[[koef|coiffe]]\n",
      "1\n",
      "t:o, tr:???\n",
      "2\n",
      "t:cxxxhoef, tr:???\n",
      "1\n",
      "t:Ne, tr:[[ne]]_[[1]]_\n",
      "yes2\n",
      "1\n",
      "t:wriont, tr:[[gwriat|cousent]]\n",
      "1\n",
      "t:ket, tr:[[ket|pas]]\n",
      "1\n",
      "t:war, tr:[[war|sur]]\n",
      "1\n",
      "t:ar, tr:[[art|le]]\n",
      "1\n",
      "t:%memes%, tr:même\n",
      "2\n",
      "t:liñser, tr:[[liñser|drap]]\n",
      "1\n",
      "t:met, tr:[[met|mais]]\n",
      "1\n",
      "t:nend, tr:[[ne]]_[[+C]]_\n",
      "1\n",
      "t:aomp, tr:[[mont|allons]]\n",
      "1\n",
      "t:ket, tr:[[ket|pas]]\n",
      "6\n",
      "t:d', tr:[[da|à]]\n",
      "6\n",
      "t:ar, tr:?\n",
      "1\n",
      "t:%memp%, tr:[[art|le]]\n",
      "2\n",
      "t:tu, tr:même\n",
      "1\n",
      "t:Hag, tr:[[&|et]]\n",
      "1\n",
      "t:amañ, tr:[[amañ|ici]]\n",
      "2\n",
      "t:%memp%, tr:même\n",
      "1\n",
      "t:bout, tr:[[bez'@|expl]]\n",
      "6\n",
      "t:', tr:[[R]]\n",
      "6\n",
      "t:h, tr:?\n",
      "1\n",
      "t:eus, tr:[[E|y.a]]\n",
      "1\n",
      "t:ur, tr:[[art|un]]\n",
      "1\n",
      "t:yocxxxh, tr:[[yocxxxh|tas]]\n",
      "1\n",
      "t:ha, tr:[[c.ha(g)|que]]_3PL_[[kaout|a]]\n",
      "6\n",
      "t:', tr:[[berr-|court]].[[anal|souffle]]\n",
      "t:', tr:[[berr\n",
      "t:', tr:-|court]]\n",
      "t:', tr:[[anal|souffle]]\n",
      "6\n",
      "t:deus, tr:?\n",
      "2\n",
      "t:berranal, tr:???\n",
      "1\n",
      "t:Eñ, tr:[[pfi|lui]]\n",
      "6\n",
      "t:', tr:[[kaout|a]]\n",
      "6\n",
      "t:deus, tr:?\n",
      "1\n",
      "t:ar, tr:[[art|le]]\n",
      "1\n",
      "t:%memp%, tr:même\n",
      "1\n",
      "t:oad, tr:[[oad|âge]]\n",
      "2\n",
      "t:genin, tr:[[gant|avec]].[[pronom_incorporé|moi]]\n",
      "t:genin, tr:[[gant|avec]]\n",
      "t:genin, tr:[[pronom_incorporé|moi]]\n",
      "1\n",
      "t:Lenn, tr:[[lenn_(V.)|lire]]\n",
      "1\n",
      "t:a, tr:[[R]]_[[1]]_\n",
      "yes2\n",
      "1\n",
      "t:ra, tr:[[ober|fait]]\n",
      "1\n",
      "t:ar, tr:[[art|le]]\n",
      "1\n",
      "t:%memes%, tr:même\n",
      "1\n",
      "t:levrioù, tr:[[levr|livre]].[[-ioù_(PL.)|s]]\n",
      "t:levrioù, tr:[[levr|livre]]\n",
      "t:levrioù, tr:[[#ioù_(PL)|s]]\n",
      "1\n",
      "t:%ha%, tr:[[C.ha(g)|que]]\n",
      "2\n",
      "t:me, tr:[[pfi|moi]]\n",
      "1\n",
      "t:Ar, tr:[[art|le]]\n",
      "1\n",
      "t:memes, tr:même\n",
      "1\n",
      "t:tra, tr:[[tra|chose]]\n",
      "2\n",
      "t:%eveldout%, tr:[[evel|comme]].[[pronom_incorporé|toi]]\n",
      "t:%eveldout%, tr:[[evel|comme]]\n",
      "t:%eveldout%, tr:[[pronom_incorporé|toi]]\n",
      "1\n",
      "t:Ne, tr:[[ne]]_[[1]]_\n",
      "yes2\n",
      "1\n",
      "t:galfecxxxh, tr:[[gallout|pourriez]]\n",
      "1\n",
      "t:ket, tr:[[ket|pas]]\n",
      "1\n",
      "t:chom, tr:[[chom|rester]]\n",
      "1\n",
      "t:aze, tr:[[aze|ici]]\n",
      "1\n",
      "t:%memes%, tr:même\n",
      "1\n",
      "t:%(la), tr:([[la(r)|que]])_[[ma(r)|si]]_[[4]]_\n",
      "yes2\n",
      "1\n",
      "t:ma%, tr:[[vez|seriez]]\n",
      "1\n",
      "t:vefecxxxh, tr:courag.[[-us|eux]]\n",
      "t:vefecxxxh, tr:courag\n",
      "t:vefecxxxh, tr:[[#us|eux]]\n",
      "2\n",
      "t:kourachus, tr:???\n",
      "10\n",
      "# sent_id = Denez(1984:73)__1\n",
      "# text = Ha merc'hed Treboull noe ket tre ar memes taol dorn da lakaat o c'hoef.\n",
      "# text_fr = 'Et les filles de Tréboul n'avaient pas la même façon de mettre leur coiffe'.\n",
      "# dialect = Cornouaillais\n",
      "# location = Douarnenez\n",
      "# source = Denez (1984)\n",
      "# texttype = Références de corpus\n",
      "1\tHa\t&.-ed_(PL)\tconjonction|complémenteur.suffixe\t_\t_\t_\t_\t_\tGloss=et.s\n",
      "2\tmerc'hed\tTréboul\tnom_propre\t_\t_\t_\t_\t_\tGloss=Tréboul\n",
      "3\tTreboull\tkaout\tauxiliaire|verbe\t_\t_\t_\t_\t_\tGloss=avait\n",
      "4\tnoe\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "5\tket\ttre\tadverbe|adjectif\t_\t_\t_\t_\t_\tGloss=tt.à.fait\n",
      "6\ttre\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "7\tar\tar\t?\t_\t_\t_\t_\t_\tGloss=même\n",
      "8\tmemes\ttaol_(M.)\tM\t_\t_\t_\t_\t_\tGloss=coup\n",
      "9\ttaol\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=pour]]|Mutation=trigger|MutationNUM=1\n",
      "10\tdorn\tlakaat\tverbe\t_\t_\t_\t_\t_\tGloss=mettre\n",
      "11\tda\tPOSS\tdéterminant\t_\t_\t_\t_\t_\tGloss=leur]]|Mutation=trigger|MutationNUM=2\n",
      "12\tlakaat\tkoef\tnom\t_\t_\t_\t_\t_\tGloss=coiffe\n",
      "13\to\to\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "14\tc'hoef\tc'hoef\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "15\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Bouzeg(1986:II)__1\n",
      "# text = Ne wriont ket war ar memes liñser.\n",
      "# text_fr = 'Ils ne sont pas du même bord politique.'\n",
      "# dialect = Cornouaillais\n",
      "# location = Riec\n",
      "# source = Bouzeg (1986)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tNe\tNe\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=ne]]|Mutation=trigger|MutationNUM=1\n",
      "2\twriont\tgwriat\tverbe\t_\t_\t_\t_\t_\tGloss=cousent\n",
      "3\tket\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "4\twar\twar\tpréposition\t_\t_\t_\t_\t_\tGloss=sur\n",
      "5\tar\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "6\tmemes\tmemes\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "7\tliñser\tliñser\tnom\t_\t_\t_\t_\t_\tGloss=drap\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Scaër/Bannalec,H.Gaudart(04/2016b)__1\n",
      "# text = Ne galfec'h ket chom aze memes (la) ma vefec'h kourachus.\n",
      "# text_fr = 'Tu n'y resterais pas, même si tu étais courageux.'\n",
      "# dialect = Cornouaillais\n",
      "# location = Scaër/Bannalec\n",
      "# source = H. Gaudart (04/2016b)\n",
      "# texttype = Élicitations\n",
      "1\tNe\tNe\tcomplémenteur\t_\t_\t_\t_\t_\tGloss=ne]]|Mutation=trigger|MutationNUM=1\n",
      "2\tgalfec'h\tgallout\tverbe\t_\t_\t_\t_\t_\tGloss=pourriez\n",
      "3\tket\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "4\tchom\tchom\tverbe\t_\t_\t_\t_\t_\tGloss=rester\n",
      "5\taze\taze\tadverbe\t_\t_\t_\t_\t_\tGloss=ici\n",
      "6\tmemes\tmemes\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "7\t(la)\t([[la(r)\tr\t_\t_\t_\t_\t_\tGloss=que]])_[[ma(r)|si]]|Mutation=trigger|MutationNUM=4\n",
      "8\tma\tvez\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=seriez\n",
      "9\tvefec'h\tvefec'h.-us\t?.suffixe|adjectif\t_\t_\t_\t_\t_\tGloss=courag.eux\n",
      "10\tkourachus\tkourachus\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "11\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Herrieu(1994:81)__1\n",
      "# text = met nend aomp ket d'ar memp tu.\n",
      "# text_fr = '...mais nous n'allons pas du même côté.'\n",
      "# dialect = Vannetais\n",
      "# source = Herrieu (1974)\n",
      "# texttype = Références de corpus\n",
      "1\tmet\tmet\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=mais\n",
      "2\taomp\tmont\tverbe\t_\t_\t_\t_\t_\tGloss=allons\n",
      "3\tket\tket\tadverbe\t_\t_\t_\t_\t_\tGloss=pas\n",
      "4\td'\tda\tpréposition\t_\t_\t_\t_\t_\tGloss=à\n",
      "5\tar\tar\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "6\tmemp\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "7\ttu\ttu\tnom\t_\t_\t_\t_\t_\tGloss=même\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Louis(2015:131)__1\n",
      "# text = Hag amañ memp, bout 'h eus ur yoc'h ha 'deus berranal.\n",
      "# text_fr = 'Et ici même, il y a beaucoup (de pensionnaires) qui souffrent d’insuffisance respiratoire.'\n",
      "# dialect = Vannetais\n",
      "# location = JMh\n",
      "# source = Louis (2015)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tHag\t&\tconjonction|complémenteur\t_\t_\t_\t_\t_\tGloss=et\n",
      "2\tamañ\tamañ\tadverbe\t_\t_\t_\t_\t_\tGloss=ici\n",
      "3\tmemp\tmemp\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "4\t,\t,\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "5\tbout\tbez'\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=expl\n",
      "6\t'\t'\t?\t_\t_\t_\t_\t_\tGloss=R\n",
      "7\th\th\t?\t_\t_\t_\t_\t_\tGloss=?\n",
      "8\teus\tE\tverbe\t_\t_\t_\t_\t_\tGloss=y.a\n",
      "9\tur\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=un\n",
      "10\tyoc'h\tyoc'h\t?\t_\t_\t_\t_\t_\tGloss=tas\n",
      "11\tha\tc.ha(g)\tg\t_\t_\t_\t_\t_\tGloss=que\n",
      "12\t'\t'-'.anal\t?-?.nom\t_\t_\t_\t_\t_\tGloss=[[berr--|court]].souffle\n",
      "13\tdeus\tdeus\tpréposition\t_\t_\t_\t_\t_\tGloss=?\n",
      "14\tberranal\tberranal\t?\t_\t_\t_\t_\t_\tGloss=???\n",
      "15\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Nicolas(2005:31)__1\n",
      "# text = Eñ 'deus ar memp oad genin.\n",
      "# text_fr = 'Il a le même âge que moi.'\n",
      "# dialect = Vannetais\n",
      "# location = Kistinid\n",
      "# source = Nicolas (2005)\n",
      "# texttype = Ouvrages de recherche\n",
      "1\tEñ\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=lui\n",
      "2\t'\tkaout\tauxiliaire|verbe\t_\t_\t_\t_\t_\tGloss=a\n",
      "3\tdeus\tdeus\tpréposition\t_\t_\t_\t_\t_\tGloss=?\n",
      "4\tar\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "5\tmemp\tmemp\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "6\toad\toad\tnom\t_\t_\t_\t_\t_\tGloss=âge\n",
      "7\tgenin\tgant.pronom_incorporé\tpréposition.pronom\t_\t_\t_\t_\t_\tGloss=avec.moi\n",
      "8\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Trégor,Kerrain(2001)__1\n",
      "# text = Lenn a ra ar memes levrioù ha me.\n",
      "# text_fr = 'Elle lit les mêmes livres que moi.'\n",
      "# dialect = Standard\n",
      "# location = Trégor\n",
      "# source = Kerrain (2001)\n",
      "# texttype = Ouvrages pédagogiques\n",
      "1\tLenn\tlenn_(V.)\tverbe\t_\t_\t_\t_\t_\tGloss=lire\n",
      "2\ta\ta\tpréposition\t_\t_\t_\t_\t_\tGloss=R]]|Mutation=trigger|MutationNUM=1\n",
      "3\tra\tober\tverbe|auxiliaire\t_\t_\t_\t_\t_\tGloss=fait\n",
      "4\tar\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "5\tmemes\tmemes\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "6\tlevrioù\tlevr.-ioù_(PL)\tnom.suffixe\t_\t_\t_\t_\t_\tGloss=livre.s\n",
      "7\tha\tC.ha(g)\tg\t_\t_\t_\t_\t_\tGloss=que\n",
      "8\tme\tpfi\tpronom\t_\t_\t_\t_\t_\tGloss=moi\n",
      "9\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n",
      "# sent_id = Trégor,Kerrain(2001)__1\n",
      "# text = Ar memes tra eveldout.\n",
      "# text_fr = 'Il a la même voiture que moi.'\n",
      "# dialect = Standard\n",
      "# location = Trégor\n",
      "# source = Kerrain (2001)\n",
      "# texttype = Ouvrages pédagogiques\n",
      "1\tAr\tart\tdéterminant|quantifieur\t_\t_\t_\t_\t_\tGloss=le\n",
      "2\tmemes\tmemes\tadjectif\t_\t_\t_\t_\t_\tGloss=même\n",
      "3\ttra\ttra\tnom\t_\t_\t_\t_\t_\tGloss=chose\n",
      "4\teveldout\tevel.pronom_incorporé\tpréposition|complémenteur.pronom\t_\t_\t_\t_\t_\tGloss=comme.toi\n",
      "5\t.\t.\tPUNCT\t_\t_\t_\t_\t_\tGloss=punct\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "{| class=\"prettytable\"\n",
    "|(1)|| N'eo || ket || dleet || koduiñ || hag || evañ || er || memes || amzer.\n",
    "|-\n",
    "||| [[ne]] [[COP|est]] || [[ket|pas]] || [[dleout|dû]] || conduire || [[&|et]] || [[evañ|boire]] || [[P.e|en]].[[art|le]] || [[memes|même]] || temps\n",
    "|-\n",
    "|||colspan=\"10\" | 'On ne doit pas boire et conduire en même temps /Il ne faut pas boire et conduire' \n",
    "|-\n",
    "|||||||||colspan=\"10\" | ''Lesneven/Kerlouan'', [[Y. M. (04/2016)]]\n",
    "|}\n",
    "\"\"\"\n",
    "\n",
    "dialects = {\"léonard\" : [], \"cornouaillais\" : [], \"trégorrois\": [], \"vannetais\" : [], \"standard\" : [], \"inconnu\" : [], \"breton central\" : []}\n",
    "references = {} # keeps track of the references of the sentences: ref:[sentence]\n",
    "sentences = {} # keeps the actual texts\n",
    "dic = {} # token to lemma, pos, gloss\n",
    "errorout = open('breton.errors.txt','w', encoding='utf-8')\n",
    "\n",
    "#document contenant la liste des mots qui n'ont pas de POS\n",
    "nopos = open('no_pos.txt','w', encoding='utf-8')\n",
    "\n",
    "currentPage=''\n",
    "\n",
    "unknownPOS = []\n",
    "\n",
    "### EXPRESSIONS REGULIERES\n",
    "repretty = re.compile(r'\\{\\|.*?class=\"prettytable\"((\\n|.)*?)\\|\\}', re.MULTILINE)\n",
    "rePOS = re.compile(r'verbes|auxiliaires|copules|adverbes|complémenteurs|conjonctions|prépositions|adjectifs|noms|particules verbales|interjections|postpositions|déterminants|quantifieurs|pronoms|noms propres|suffixe', re.IGNORECASE)\n",
    "reDialect = re.compile(r'léonard|cornouaillais|trégorrois|vannetais|breton central|standard', re.IGNORECASE)\n",
    "reLanguage = re.compile(r'gallois|roumain|italien|espagnol|hébreu|arabe|français|anglais|allemand|moyen breton|vieux breton|breton pré-moderne', re.IGNORECASE)\n",
    "reType = re.compile(r'ouvrages de recherche|références de corpus|élicitations|ouvrages pédagogiques|dictionnaires|grammaires', re.IGNORECASE)\n",
    "reNoms = re.compile(r'Yann|Marijo|Anna|Angela|Alejandro|Odile|Anastazi|Nolwenn|Solange|Bastien|Soazig|Marie|Marsel|Lukaz|Lenaig|Kristof|Julie|Jenovefa|Simone|Mona|Mari', re.IGNORECASE)\n",
    "relinks = re.compile(r\"\\[\\[(.*?)\\]\\]\")\n",
    "relinks2 = re.compile(r\"\\[\\[.*?\\]\\]\")  \n",
    "reAlternatives = re.compile(r'\\([^\\W\\d_].* */ *[^\\W\\d_].*\\)', re.U)\n",
    "reWord = re.compile(r'\\[\\[[a-z]+')\n",
    "nospace = [\"numéraux cardinaux\",\"nom propre\", \"pronom incorporé\", \"nom de titre\", '(', \"-,\", 'pronom réfléchi', \"nom nu\", \", -\", \"pluriel interne\", \"Les pronoms d'@incise contrastifs\", \"particule o\"]  \n",
    "categories = {\"V\": \"verbe\", \"Adj\": \"adjectif\", \"PL\": \"suffixe\"}\n",
    "\n",
    "#print(pages['Konduiñ'])\n",
    "#for key in pages:\n",
    "\t#if key.startswith('A'):\n",
    "\t\t#print(key)\n",
    "\n",
    "# Recherche du pos dans une page titre\n",
    "# la fontion cherche le pos dans toutes les pages selon le lemme donné et retourne le pos associé\n",
    "def getFirstPos(title, currentPage):\n",
    "\tif not title: return '?'\n",
    "\tmpos='' \n",
    "\tif title.isupper() and title != 'R':\n",
    "\t\tmpos = title   \n",
    "\tif title == '[[R]]' :   \n",
    "\t\tmpos='particule_verbale'\n",
    "\tif title == 'P.e' or 'P/e':\n",
    "\t\tmpos='préposition' \n",
    "\tif re.match(r\".*\\((.*)\\)\", title):                    \n",
    "\t\ttest_pos = re.match(r\".*\\((.*)\\)\", title).group(1)                   \n",
    "\t\tif test_pos.replace(\".\", \"\") in categories:                       \n",
    "\t\t\tmpos = categories[test_pos.replace(\".\", \"\")]\n",
    "\t\telse: \n",
    "\t\t\tmpos = test_pos.replace('.', '')            \n",
    " \n",
    "\telse:        \n",
    "\t#si le titre contient un tiret bas, on le remplace par un espace pour le trouver dans les titres de pages\n",
    "\t\tif '_' in title: \n",
    "\t\t\ttitle = title.replace(\"_\", \" \")\n",
    "\t\tif '#' in title: \n",
    "\t\t\ttitle = title.replace(\"#\", \"-\")            \n",
    "#sinon on met une majuscule à la première lettre du titre\n",
    "\t\telse:             \n",
    "\t\t\ttitle = title[0].upper()+title[1:]\n",
    "\t#si le titre est contenu dans le dictionnaire pages\n",
    "\t\tif title in pages:           \n",
    "\t\t\twikicode=''\n",
    "\t\t\t#gérer les redirections (parfois les exemples se trouvent sur d'autres pages)\n",
    "\t\t\tif '#REDIRECTION' in pages[title]:             \n",
    "\t\t\t\t#on suit la redirection\n",
    "\t\t\t\tnewtitle = relinks.search(pages[title]).group(1)                \n",
    "\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\tif '_' in newtitle: \n",
    "\t\t\t\t\tnewtitle = newtitle.replace(\"_\", \" \")            \n",
    "\t\t\t\t#et on regarde à nouveau si le titre est dans le dictionnaire pages\n",
    "\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\twikicode = pages[newtitle]              \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t# print(8888,newtitle, newtitle in pages)\n",
    "\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:].replace(' ','_')\n",
    "\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnewtitle = newtitle.split(',')[0]\n",
    "\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\twikicode = pages[newtitle]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\terrorout.write('\\n'+currentPage+': strange redirect to page that does not exist: '+newtitle+'\\n')\n",
    "\t\t\telse:\n",
    "\t\t\t\twikicode = pages[title]                               \n",
    "\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "\t\t\t\tcats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)\n",
    "\t\t\t\tlist_cats = set()\n",
    "\t\t\t\tfor cat in cats:\n",
    "\t\t\t\t\tif rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "\t\t\t\t\t\tlist_cats.add(rePOS.search(cat).group(0).rstrip('s'))\n",
    "\t\t\t\t\t\tif len(list_cats) == 1:                                          \n",
    "\t\t\t\t\t\t\tmpos = list(list_cats)[0]\n",
    "\t\t\t\t\t\telif len(list_cats) >0: \n",
    "\t\t\t\t\t\t\tmpos = \"|\".join(list_cats)                            \n",
    "\t\t\tif mpos:\n",
    "\t\t\t\tmpos = mpos\n",
    "\t\t\telse:\n",
    "\t\t\t\tmpos = \"?\"\n",
    "\t\telse:          \n",
    "\t\t\tif reNoms.search(title):             \n",
    "\t\t\t\tmpos = \"nom_propre\"\n",
    "\t\t\telif \"%\" in title:             \n",
    "\t\t\t\twikicode = pages[currentPage]                               \n",
    "\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", wikicode):\n",
    "\t\t\t\t\tcats = re.findall(r\"\\[\\[Category:.*\\|\", wikicode)\n",
    "\t\t\t\t\tlist_cats = set()\n",
    "\t\t\t\t\tfor cat in cats:\n",
    "\t\t\t\t\t\tif rePOS.search(cat) and rePOS.search(cat).group(0) not in list_cats:\n",
    "\t\t\t\t\t\t\tlist_cats.add(rePOS.search(cat).group(0).rstrip('s'))\n",
    "\t\t\t\t\t\t\tif len(list_cats) == 1:                                          \n",
    "\t\t\t\t\t\t\t\tmpos = list(list_cats)[0]\n",
    "\t\t\t\t\t\t\telif len(list_cats) >0: \n",
    "\t\t\t\t\t\t\t\tmpos = \"|\".join(list_cats)                               \n",
    "\t\t\telse:               \n",
    "\t\t\t\tmpos = \"?\"                   \n",
    "\treturn mpos\n",
    "\n",
    "\n",
    "# Suppression des espaces dans les [[ ]] sinon, le split avec comme délimiteur espace va mal se passer...\n",
    "# case : [[koll|perd]].[[-et (Adj.)|u]]\n",
    "## ATTENTION SI DEUX ESPACES DANS UN MEME [[ ]] n'enlèvera que le premier, à améliorer !!\n",
    "def sansespace(ch):\n",
    "\tfor e in relinks.finditer(ch):\n",
    "\t\tif \" \" in e.group(0):\n",
    "\t\t\tstart, end = e.start(), e.end()\n",
    "\t\t\tnewch = re.sub(r\"\\[\\[(.*?) (.*?)\\]\\]\", \"[[\\\\1_\\\\2]]\", e.group(0))\n",
    "\t\t\tnewch = newch.replace(' ', '_') \n",
    "\t\t\tch = ch[:start] + newch + ch[start+len(newch)+1:]\n",
    "\treturn ch\n",
    "#exemple :\n",
    "#print(sansespace(\"[[art|le]] _[[1]]_[[hey j]]_[[maouez | femme]]\"))\n",
    "#resultat : \"[[art|le]] _[[1]]_[[heyj]]_[[maouez| femme]]\")\n",
    "#print(sansespace(\"[[R]] [[+C]] [[ez eus|est]]\"))\n",
    "\n",
    "##  _[[1]]_exemple pas pris en compte\n",
    "mutation1 = re.compile(r\"_\\[\\[([12345])\\]\\]_\\[\\[(.*)\\]\\]\")\n",
    "mutation2 = re.compile(r\"\\[?\\[?(.*)\\]?\\]?_\\[\\[([12345])\\]\\]_\")\n",
    "\n",
    "#ex: \"[[ne]]_[[+C]]_\"\n",
    "epenthetique1 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[ma|que]]_[[4]],+C_\n",
    "epenthetique2 = re.compile(r\"\\[\\[(.*)\\]\\]_\\[\\[([12345])\\]\\],(\\[?\\[?\\+C\\]?\\]?)_\")\n",
    "#ex: [[R]]_[[+C]],[[4]]_  \n",
    "epenthetique3 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?),\\[\\[([12345])\\]\\]_\")\n",
    "#ex : [[R]]_[[+C]]_[[ez eus|est]]\n",
    "#epenthetique4 = re.compile(r\"\\[\\[(.*)\\]\\]_(\\[?\\[?\\+C\\]?\\]?)_[\\[(.*)\\]\\]\")\n",
    "#test = \"_[[1]]_[[kozh|vieille]]\"\n",
    "#test = \"([[R]]_[[1]]_)\"\n",
    "#print(mutation2.search(test))  \n",
    "mutation = False\n",
    "epenthetique = False\n",
    "\n",
    "groupes_pipe = re.compile(r\"(.*?)\\|(.*)\")\n",
    "# fonction qui prend un token, avec ou sans double crochets [[ ]]\n",
    "# NB : tr = traduction ; t = token\n",
    "\n",
    "def tokentrans2lemposglossmorph(tr, t, title, currentPage):\n",
    "\tprint(f't:{t}, tr:{tr}')         \n",
    "\t# Rq : morph a par défaut '_'\n",
    "\tlem, pos, gloss, morph = '', '', '', ''\n",
    "\tm = relinks.search(tr)     \n",
    "\ttest_mutation1 = mutation1.search(tr)\n",
    "\ttest_mutation2 = mutation2.search(tr)\n",
    "\ttest_epenthetique1 = epenthetique1.search(tr)  \n",
    "\ttest_epenthetique2 = epenthetique2.search(tr)  \n",
    "\ttest_epenthetique3 = epenthetique3.search(tr)\n",
    "    \n",
    "    \n",
    "    # Si c'est une mutation\n",
    "\tif test_mutation1 : \n",
    "\t\tprint(\"yes1\")        \n",
    "\t\tmutation = True        \n",
    "\t\tif ('.[[' or ']].') in tr:\n",
    "\t\t\ttr1 = \"[[\" + test_mutation1.group(2) + \"]]\"          \n",
    "\t\t\tlems, glosss, poss = [], [], []\n",
    "\t\t\tfor mbis in relinks2.finditer(tr1):                  \n",
    "\t\t\t\ttry:                \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(mbis.group(0), t, title, currentPage)\n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem, currentPage)]  \n",
    "\t\t\t\t\tmorph = morph + test_mutation1.group(1) + \" \" + \"target\"                    \n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\twith open('abc.txt','w', encoding=\"utf-8\") as f :                \n",
    "\t\t\t\t\t\tf.write(f\"PROBLEMEEEEEEEEEEEEE : {tr} et {t}\") \n",
    "\t\t\t\tlem = '.'.join(lems)\n",
    "\t\t\t\tgloss = '.'.join(glosss)\n",
    "\t\t\t\tpos = '.'.join(poss)\n",
    "\t\t\treturn lem, pos, gloss, morph\n",
    "        \n",
    "\t\telse:          \n",
    "\t\t\tmorph = test_mutation1.group(1) + \" \" + \"target\"\n",
    "\t\t\ttoken_mute = test_mutation1.group(2)       \n",
    "\t\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\t\tif token_mute_pipe: \n",
    "\t\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\t\tif gloss == 'R':  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:   \n",
    "\t\t\t\t\tpos = getFirstPos(lem, currentPage)\n",
    "\t\t\telse :\n",
    "\t\t\t\tlem = t        \n",
    "\t\t\t\tgloss = token_mute   \n",
    "\t\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:                          \n",
    "\t\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\t\tif pos == '?':\n",
    "\t\t\t\t\tpos = getFirstPos(tr, currentPage)\n",
    "                    \n",
    "                    \n",
    "\telif test_mutation2 :\n",
    "\t\tprint(\"yes2\")  \n",
    "\t\tmutation = True        \n",
    "\t\tif ('.[[' or ']].') in tr:                \n",
    "\t\t\ttr1 = \"[[\" + test_mutation2.group(2) + \"]]\"               \n",
    "\t\t\tlems, glosss, poss = [], [], []\n",
    "\t\t\tfor mbis in relinks2.finditer(tr1):            \n",
    "\t\t\t\ttry:                \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(mbis.group(0), t, title, currentPage)\n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem, currentPage)]\n",
    "\t\t\t\t\tmorph = morph + test_mutation2.group(2) + \" \" + \"trigger\"\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\twith open('abc.txt','w', encoding=\"utf-8\") as f :                \n",
    "\t\t\t\t\t\tf.write(f\"PROBLEMEEEEEEEEEEEEE : {tr} et {t}\") \n",
    "\t\t\t\tlem = '.'.join(lems)\n",
    "\t\t\t\tgloss = '.'.join(glosss)\n",
    "\t\t\t\tpos = '.'.join(poss)\n",
    "\t\t\treturn lem, pos, gloss, morph\n",
    "        \n",
    "\t\telse:              \n",
    "\t\t\tmorph = test_mutation2.group(2) + \" \" + \"trigger\"\n",
    "\t\t\ttoken_mute = test_mutation2.group(1)       \n",
    "\t\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)          \n",
    "\t\t\tif token_mute_pipe:      \n",
    "\t\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\t\telse:                \n",
    "\t\t\t\t\tpos = getFirstPos(lem, currentPage)\n",
    "\t\t\telse :\n",
    "\t\t\t\tlem = t\n",
    "\t\t\t\tgloss = token_mute\n",
    "\t\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\"            \n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\t\tif pos == '?':\n",
    "\t\t\t\t\tpos = getFirstPos(tr, currentPage)\n",
    "#S'il y a une consonne épenthétique (+C)             \n",
    "\telif test_epenthetique1 : \n",
    "\t\tepenthetique = True    \n",
    "\t\t#print(\"yes3\") \n",
    "\t\tmorph = test_epenthetique1.group(2) \n",
    "\t\ttoken_mute = test_epenthetique1.group(1)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, currentPage)\n",
    "\t\telse :\n",
    "\t\t\t# print('just a word')\n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, currentPage)\n",
    "\n",
    "\telif test_epenthetique2 : \n",
    "\t\tepenthetique = True    \n",
    "\t\tmorph = test_epenthetique2.group(2) + '|' +  test_epenthetique2.group(3) \n",
    "\t\ttoken_mute = test_epenthetique2.group(1)    \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, currentPage)\n",
    "\t\telse :\n",
    "\t\t\t# print('just a word')\n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, currentPage)\n",
    "                             \n",
    "\telif test_epenthetique3:\n",
    "\t\tepenthetique = True    \n",
    "\t\tmorph = test_epenthetique3.group(2) + '|' +  test_epenthetique3.group(3)\n",
    "\t\ttoken_mute = test_epenthetique3.group(1)       \n",
    "\t\ttoken_mute_pipe = groupes_pipe.search(token_mute)\n",
    "\t\tif token_mute_pipe:      \n",
    "\t\t\tlem = token_mute_pipe.group(1)\n",
    "\t\t\tgloss = token_mute_pipe.group(2)\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\" \n",
    "\t\t\telse:                \n",
    "\t\t\t\tpos = getFirstPos(lem, currentPage)\n",
    "\t\telse :\n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = token_mute\n",
    "\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"                  \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, currentPage)                    \n",
    "                               \n",
    "# si ce n'est pas une mutation et si on a des double crochets [[ ]]              \n",
    "\telif m:        \n",
    "\t\t# cas \"amalgames\" et/ou mots contenant un tiret: exemple [[da|à]].[[pronom incorporé|vous]] ou [[den|parent]].[[pluriel interne|s]]-[[kozh|vieil]]  \n",
    "\t\tif ('.[[' or ']].' or '-[[' or ']]-' ) in tr:      \n",
    "\t\t\tlems, glosss, poss, ponct = [], [], [], [] \n",
    "\t\t\tlemt, glosst, post = [], [], []\n",
    "\t\t\tif '(Adj.)' or '(V.)' or '(PL.)' or 'P.e' or '(F.)' in tr:      \n",
    "\t\t\t\ttr = tr.replace('(Adj.)', '(Adj)').replace('(V.)', '(V)').replace('(PL.)', '(PL)').replace('P.e', 'P/e').replace('(F.)', '(F)')   \n",
    "\t\t\t#s'il y a des tirets dans les groupes entre crochets, on les remplace par le signe #        \n",
    "\t\t\t#ex: [[genel|nat]].[[-idig|if]]  -->  [[genel|nat]].[[#idig|if]]             \n",
    "\t\t\tif re.search(r\"\\[ ?- ?[A-Za-z]\", tr):               \n",
    "\t\t\t\tsuffixe = re.search(r\"\\[ ?(-) ?[A-Za-z]\", tr).group(1)\n",
    "\t\t\t\ttr = re.sub(r'(?<=\\[) ?- ?(?=[A-Za-z])', '#', tr )\n",
    "\t\t\ttr = tr.replace(\".\", \" .\").replace(\"-\", \" -\")                    \n",
    "\t\t\tfor i in range(len(tr.split())):         \n",
    "\t\t\t\ttest = tr.split()[i]    \n",
    "\t\t\t\tif relinks2.search(test):                 \n",
    "\t\t\t\t\tele = relinks2.search(test).group(0)                                        \n",
    "\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(ele, t, title, currentPage)   \n",
    "\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\tposs += [getFirstPos(lem, currentPage)]\n",
    "\t\t\t\t\t# except:\n",
    "\t\t\t\t\t# \tprint(\"error\")                        \n",
    "\t\t\t\t\t# \twith open('abc.txt','w', encoding=\"utf-8\") as f :                \n",
    "\t\t\t\t\t# \t\tf.write(f\"PROBLEMEEEEEEEEEEEEE : {tr} et {t}\") \n",
    "\t\t\t\telse:                                  \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(test, t, title, currentPage)                 \n",
    "\t\t\t\t\t\tlems += [lem]\n",
    "\t\t\t\t\t\tglosss += [gloss]\n",
    "\t\t\t\t\t\tposs += [getFirstPos(lem, currentPage)]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\twith open('abc.txt','w', encoding=\"utf-8\") as f :                \n",
    "\t\t\t\t\t\t\t\tf.write(f\"PROBLEMEEEEEEEEEEEEE : {tr} et {t}\")  \n",
    "\t\t\t\t#on \"recolle\" les éléments avec le bon séparateur (tiret ou point)                                \n",
    "\t\t\t\tif tr.split()[i].startswith(\".\"): \n",
    "\t\t\t\t\t#print(f\"point : {tr.split()[i]}, {lems[i]}, {glosss[i]}, {poss[i]}\")\n",
    "\t\t\t\t\tlemt.append(f\".{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\".{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\".{poss[i]}\")  \n",
    "\t\t\t\telif tr.split()[i].startswith(\"-\"): \n",
    "\t\t\t\t\t#print(f\"tiret : {tr.split()[i]}, {lems[i]}, {glosss[i]}, {poss[i]}\")\n",
    "\t\t\t\t\tlemt.append(f\"-{lems[i].replace('#', '-')}\")\n",
    "\t\t\t\t\tglosst.append(f\"-{glosss[i]}\")  \n",
    "\t\t\t\t\tpost.append(f\"-{poss[i]}\")  \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\t#print(f\"rien : {tr.split()[i]}, {lems[i]}, {glosss[i]}, {poss[i]}\") \n",
    "\t\t\t\t\tlemt.append(lems[i].replace('#', '-')) \n",
    "\t\t\t\t\tglosst.append(glosss[i])                    \n",
    "\t\t\t\t\tpost.append(poss[i])                                        \n",
    "                        \n",
    "            \n",
    "\t\t\t# lem = '-'.join(lems)\n",
    "\t\t\t# gloss = '-'.join(glosss)\n",
    "\t\t\t# pos = '-'.join(poss)\n",
    "            \n",
    "\t\t\tlem = ''.join(lemt)\n",
    "\t\t\tgloss = ''.join(glosst)\n",
    "\t\t\tpos = ''.join(post)             \n",
    "        \n",
    "\t\t\t#print('__lem, pos, gloss:', lem, pos, gloss)\n",
    "\t\t\treturn lem, pos, gloss, morph        \n",
    "        \n",
    "        \n",
    "        \n",
    "\t\t# cas \"normal\" : pas d'amalgame, un token et une gloss --> [[token|gloss]]\n",
    "\t\tif '|' in m.group(1):      \n",
    "\t\t\tif \"numéraux_cardinaux\" in m.group(1): \n",
    "\t\t\t\tlem = t\n",
    "\t\t\t\tpos = m.group(1).split('|')[0]\n",
    "\t\t\t\tgloss = m.group(1).split('|')[1] \n",
    "\t\t\telse:                 \n",
    "\t\t\t\tlem = m.group(1).split('|')[0]\n",
    "\t\t\t\tgloss = m.group(1).split('|')[1]                  \n",
    "\t\t\t\tif gloss == 'R' or '[[R]' in gloss:  \n",
    "\t\t\t\t\tpos = \"particule_verbale\"                      \n",
    "\t\t\t\telif lem == 'P.e':  \n",
    "\t\t\t\t\tlem = \"e\"\n",
    "\t\t\t\t\tpos = \"préposition\"               \n",
    "\t\t\t\telif lem == 'nom_propre':  \n",
    "\t\t\t\t\tpos = lem\n",
    "\t\t\t\t\tlem = gloss                 \n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpos = getFirstPos(lem, currentPage)                        \n",
    "\t\t\t\tif pos=='?':\n",
    "\t\t\t\t# ?\n",
    "\t\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "                \n",
    "\t\t# cas sans amalgame mais sans gloss --> [[ne]]\n",
    "\t\telse:            \n",
    "\t\t\tlem = t\n",
    "\t\t\tgloss = relinks.search(tr).group(1)\n",
    "\t\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\t\tpos = \"particule_verbale\"\n",
    "\t\t\tif 'DIM' in gloss and  t.endswith('ig') or t.endswith('ig%'):\n",
    "\t\t\t\tlem = \"-ig\"                \n",
    "\t\t\t\tpos = \"suffixe\"                \n",
    "\t\t\telse:\n",
    "\t\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, currentPage)      \n",
    "\t# cas d'un mot seul sans crochets --> foot dans la page \"Abardaez,_enderv\"      \n",
    "\telse :\n",
    "#\t\tprint('just a word')\n",
    "\t\tlem = t\n",
    "\t\tgloss = tr\n",
    "\t\tif gloss == 'R'  or '[[R]' in gloss:  \n",
    "\t\t\tpos = \"particule_verbale\"            \n",
    "\t\telse:\n",
    "\t\t\tpos = getFirstPos(t, currentPage)\n",
    "\t\t\tif pos == '?':\n",
    "\t\t\t\tpos = getFirstPos(tr, currentPage)\n",
    "\t#print('__lem, pos, gloss:', lem, pos, gloss)\n",
    "\treturn lem, pos, gloss, morph\n",
    "\n",
    "\n",
    "# Nettoyage des tokens\n",
    "def cleanToken(t):\n",
    "    \n",
    "\t## ATTENTION ''' ''' veut dire que le pos du token est sur la page même ! peut être à utiliser..      \n",
    "\tt = t.replace(\"'''\", \"%\")\n",
    "\tt = t.replace(\"''\", \"\")\n",
    "\tt = re.sub(r'</?font.*?>','',t)\n",
    "\tt = re.sub(r'<sup>','_',t)\n",
    "\tt = re.sub(r'</sup>','_',t)\n",
    "\tt = re.sub(r'<u>','',t)\n",
    "\tt = re.sub(r'</u>','',t)\n",
    "\tt = re.sub(r'<sub>.*?</sub>','',t)\n",
    "\tt = re.sub(r'\\(\\[\\[\\*\\]\\].*?\\)','$$$',t)\n",
    "\tt = re.sub(r\"c'h\",'cxxxh',t)\n",
    "\tt = re.sub(r\"C'h\",'Cxxxh',t)\n",
    "\tt = re.sub(r\"`\",'',t) \n",
    "\tt = re.sub(r\"#\",'-',t)     \n",
    "\t## ne pas splitter quand l'apostrophe est en début de mot (Rannig) -> pas réussi avec la regex    \n",
    "\t#if t.startswith(\"'\"): \n",
    "\t\t#print(f\"it's heere {t}\")        \n",
    "\t\t#t = re.sub(r\"'([^ ])\",\"%\\\\1\",t)\n",
    "        \n",
    "\t# pour spliter sur les apostrophes sans les enlever : mettre @ pour spliter sur @ et donc conserver l'apostrophe\n",
    "\t##ATTENTION guillemet en début de mot puis token collés mal fait \n",
    "\tt = re.sub(r\"'\",\"'@\",t)\n",
    "\treturn t.strip()\n",
    "\n",
    "#test\n",
    "#print(cleanToken(\"'anava\"))\n",
    "\n",
    "# Remettre les tokens qui ont été remplacés\n",
    "def remettretoken(t):\n",
    "\tt = re.sub(r\"%\",\"\",t)\n",
    "\tt = re.sub(r'cxxxh',\"c'h\",t)\n",
    "\tt = re.sub(r'Cxxxh',\"C'h\",t)\n",
    "\t#t = re.sub(r\"%\",\"'\",t)\n",
    "\tt = re.sub(r\"'@\", \"'\",t)\n",
    "\tt = re.sub(r\"\\$\\$\\$ \", \"\",t)\n",
    "\treturn t\n",
    "\n",
    "# Savoir si une string contient un nombre\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def analyzePage(title): \n",
    "\tcurrentPage = title\n",
    "\tmpos = rePOS.search(pages[title])\n",
    "\tpagepos = None\n",
    "\tconlls=[]\n",
    "\tgrammatical = True\n",
    "\ts= ''\n",
    "\tif mpos:\n",
    "\t\t# mets tout en minuscule\n",
    "\t\tpagepos = mpos.group(0).lower()\n",
    "\t\t# print('pagepos',pagepos)\n",
    "\t#pour chaque tableau (et donc chaque exemple)\n",
    "\tnb_tables = 0    \n",
    "\tfor m in repretty.finditer(pages[title]):\n",
    "\t\tnb_tables += 1        \n",
    "\t\t# print('\\n\\n_______________',m.group(1))\n",
    "\t\ttokens, trans = [], []\n",
    "\t\ttranslation, source, source2 = '', '', ''\n",
    "\t\tlocation, dialect, phonetic, texttype, l = '','','', '', ''\n",
    "\t\tgrammatical=True         \n",
    "\t\t# si le tableau est \"standard\" avec une ligne de token, une ligne de glose et une ligne source\n",
    "\t\tif (len(m.group(1).split('\\n'))) == 9 :\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table\n",
    "\t\t\t\tif li[:2]=='|(' :                  \n",
    "\t\t\t\t\tif '[[*]]' in li.split('||')[0]:\n",
    "\t\t\t\t\t\tgrammatical = False                       \n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]]   \n",
    "\t\t\t\t\tif tokens[0]==\"[[*]]\" :\n",
    "\t\t\t\t\t\tgrammatical = False\n",
    "\t\t\t\t\t\tbreak      \n",
    "\t\t\t\t\tif '[' in tokens[0]:\n",
    "\t\t\t\t\t\ttokens[0] = tokens[0].replace('[', '').strip()\n",
    "\t\t\t\t\t\t#phonetic[0] = phonetic[0].replace('[', '')         \n",
    "\t\t\t\t\tif ']' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '').strip()\n",
    "\t\t\t\t\t\t#phonetic[-1] = phonetic[-1].replace(']', '')\n",
    "\t\t\t\t\t#MODIF !!!                                       \n",
    "                        \n",
    "\t\t\t\tif li[:3]=='|||' and grammatical:\n",
    "\t\t\t\t\tif not trans: #la première fois = la traduction                      \n",
    "\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]  \n",
    "\t\t\t\t\t\t#print(f\"là : {trans}\")                    \n",
    "\t\t\t\t\t\tfor i in range(len(trans)):\n",
    "\t\t\t\t\t\t\tif any(ele in trans[i] for ele in nospace):                               \n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\" \", \"_\")                                \n",
    "\t\t\t\t\t\t\tif \"jours de la semaine\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\ttrans[i] = sansespace(trans[i]) \n",
    "\t\t\t\t\t\t\t#voir comment faire car peut causer des problèmes: \n",
    "\t\t\t\t\t\t\t#ex: [[R]]_[[1]]_ [[COP|sera]]                            \n",
    "\t\t\t\t\t\t\t# if \"_ \" in trans[i]:\n",
    "\t\t\t\t\t\t\t# \ttrans[i] = trans[i].replace(\"_ \", \"_\")\n",
    "\t\t\t\t\t\t#print(f\"après : {trans}\")                                \n",
    "\t\t\t\t\t\t# print(len(tokens), len(tokens)==len(trans))\n",
    "\t\t\t\t\telif 'colspan=' in li:                  \n",
    "\t\t\t\t\t\t# vu que les deux lignes translation et source remplissent cette condition, quand on arrive à source\n",
    "\t\t\t\t\t\t# la string translation est remplie donc on n'écrase pas et on passe à source\n",
    "\t\t\t\t\t\tif not translation:\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\t\tfor i in range(len(trans)): \n",
    "\t\t\t\t\t\t\t\tif any(ele in trans[i] for ele in nospace):\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\" \", \"_\")     \n",
    "\t\t\t\t\t\t\t\tif \"jours de la semaine\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\t\ttrans[i] = sansespace(trans[i]) \n",
    "\t\t\t\t\t\t\t# if \"_ \" in trans[i]:\n",
    "\t\t\t\t\t\t\t# \ttrans[i] = trans[i].replace(\"_ \", \"_\")\n",
    "                                \n",
    "\t\t\t\t\t\telif not source: # si déjà trans, alors on a la source\n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'').replace(' ','')                           \n",
    "\t\t\t\t\t\t\tif reLanguage.search(source):                              \n",
    "\t\t\t\t\t\t\t\tgrammatical = False     \n",
    "\t\t\t\t\t\t\t\tbreak                                \n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", li):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", li):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif reDialect.search(li): \n",
    "\t\t\t\t\t\t\t\tdialect = reDialect.search(li)[0]\n",
    "\t\t\t\t\t\t\tif re.search(r\"\\[\\[(.*?)\\]\\]\", li):                                     \n",
    "\t\t\t\t\t\t\t\tl = re.search(r\"\\[\\[(.*?)\\]\\]\", li).group(0) \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                   \n",
    "\t\t\t\t\t\t\t\tl = str(l).split('|')[0].replace('[[', '').replace(']]', '').replace(\"'\", \"\")\n",
    "\t\t\t\t\t\t\t\tif l in pages:                                 \n",
    "\t\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                      \n",
    "\t\t\t\t\t\t\t\t\t\t#on suit la redirection                                        \n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)\n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\t\ts = l                                    \n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                   \n",
    "\t\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "\t\t\t\t\t\t\t\t\t\tif not dialect: \n",
    "\t\t\t\t\t\t\t\t\t\t\tif reDialect.search(source):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = reDialect.search(source).group(0)                               \n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\ts = l\n",
    "\t\t\t\t\t\t\tif not location:\n",
    "\t\t\t\t\t\t\t\tif re.search(\"(''.*'')\", li):                                \n",
    "\t\t\t\t\t\t\t\t\ttest = re.search(\"''(.*)''\", li).group(1) \n",
    "\t\t\t\t\t\t\t\t\tif not reDialect.search(test) and not test.endswith(\"ais\"):                                     \n",
    "\t\t\t\t\t\t\t\t\t\tlocation = test                                   \n",
    "\t\t\t\t\t\t\t# print(111,source)\n",
    "\t\t\t# print(tokens, trans, translation, source)\n",
    "\t\t\tif not( tokens and trans and translation and source): # if one is missing\n",
    "\t\t\t\t#if verbose: print('Houston, we got a problem: The number of tokens and the number of transcriptions differ, or translation or source are missing:', title) \n",
    "\t\t\t\terrorout.write('\\n'+currentPage+': The number of tokens and the number of transcriptions differ or translation or source are missing:'+m.group(1)+'\\n')\n",
    "\t\t\t\tcontinue\n",
    "\t\t# S'il y a plus de lignes que prévu : par ex une ligne de phonétique ou deux lignes source, ou les deux\n",
    "\t\telse:\n",
    "\t\t\tfor li in m.group(1).split('\\n'): # for every line of the table\n",
    "\t\t\t\t#si ce n'est pas une ligne de phonétique                \n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==False:\n",
    "\t\t\t\t\ttokens = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\t\t#tokens[0] = tokens[0].replace('[', '')         \n",
    "\t\t\t\t\tif ']' in tokens[-1]: \n",
    "\t\t\t\t\t\ttokens[-1] = tokens[-1].replace(']', '')\n",
    "                        \n",
    "\t\t\t\t#si c'est une ligne de phonétique\n",
    "\t\t\t\tif li[:2]=='|(' and bool(re.search(r'</?font.*?>',li))==True:\n",
    "\t\t\t\t\tphonetic = [cleanToken(t) for t in li.split('||')[1:]]\n",
    "\t\t\t\t\tclean_phonetic = ''\n",
    "\t\t\t\t\tif '[' or '/' in phonetic[0]: \n",
    "\t\t\t\t\t\tphonetic[0] = phonetic[0].replace('[', '').replace('/', '')         \n",
    "\t\t\t\t\tif ']' or '/' in phonetic[-1]: \n",
    "\t\t\t\t\t\tphonetic[-1] = phonetic[-1].replace(']', '').replace('/', '')\n",
    "\t\t\t\t\tfor ele in phonetic:        \n",
    "\t\t\t\t\t\tclean_phonetic += ele\n",
    "\t\t\t\t\t\tclean_phonetic += ' '\n",
    "\n",
    "\t\t\t\tif li[:3]=='|||':\n",
    "\t\t\t\t\t# si on a rencontré une ligne phonétique et donc qu'on a pas encore de tokens, cette ligne (2e ligne) est la ligne de tokens                   \n",
    "\t\t\t\t\tif not tokens :\n",
    "\t\t\t\t\t\ttokens = [cleanToken(t) for t in li[3:].split('||')]                \n",
    "\t\t\t\t\t\t#print(tokens)\n",
    "\t\t\t\t\telif not trans: #la première fois = la traduction\n",
    "\t\t\t\t\t\ttrans = [cleanToken(tr) for tr in li[3:].split('||')]\n",
    "\t\t\t\t\t\t#print(f\"ici trans {trans}\")                        \n",
    "\t\t\t\t\t\tfor i in range(len(trans)):                            \n",
    "\t\t\t\t\t\t\tif any(ele in trans[i] for ele in nospace):\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\" \", \"_\")     \n",
    "\t\t\t\t\t\t\tif \"jours de la semaine\" in trans[i]:\n",
    "\t\t\t\t\t\t\t\ttrans[i] = sansespace(trans[i]) \n",
    "\t\t\t\t\t\t\tif \"_ \" in trans[i]:\n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].replace(\"_ \", \"_\")\n",
    "\t\t\t\t\telif 'colspan=' in li:\n",
    "\t\t\t\t\t\tif li.startswith('|||colspan='):\n",
    "\t\t\t\t\t\t\ttranslation = li.split('|')[-1]\n",
    "\t\t\t\t\t\telif not source:                        \n",
    "\t\t\t\t\t\t\tsource = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')   \n",
    "\t\t\t\t\t\t\tif reLanguage.search(li):\n",
    "\t\t\t\t\t\t\t\tgrammatical = False                             \n",
    "\t\t\t\t\t\telif not source2:                            \n",
    "\t\t\t\t\t\t\tsource2 = li.split('|')[-1].replace('[[','').replace(']]','').replace(\"''\",'')    \n",
    "\t\t\t\t\t\t\tif reLanguage.search(li):\n",
    "\t\t\t\t\t\t\t\tgrammatical = False \n",
    "\t\t\t\t\t\tif source:             \n",
    "\t\t\t\t\t\t\tif source2 : \n",
    "\t\t\t\t\t\t\t\tsource = source2                                \n",
    "\t\t\t\t\t\t\t#source, source2 = source2, source\n",
    "\t\t\t\t\t\t\tif re.findall(\"''.*''\", li):                 \n",
    "\t\t\t\t\t\t\t\tif re.search(r\"\\)''\", li):           \n",
    "\t\t\t\t\t\t\t\t\tlocation = re.findall(r\"\\(([^\\)]+)\\)\", li)[0]\n",
    "\t\t\t\t\t\t\t\t\tif has_numbers(location):\n",
    "\t\t\t\t\t\t\t\t\t\tlocation = ''\n",
    "\t\t\t\t\t\t\tif reDialect.search(li): \n",
    "\t\t\t\t\t\t\t\tdialect = reDialect.search(li)[0]\n",
    "\t\t\t\t\t\t\tif re.search(r\"\\[\\[(.*?)\\]\\]\", li):                                \n",
    "\t\t\t\t\t\t\t\tl = re.search(r\"\\[\\[(.*?)\\]\\]\", li).group(0)                              \n",
    "\t\t\t\t\t\t\t\tlang = re.search(r\"(.*?)\\[\\[\", li).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(lang): \n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                   \n",
    "\t\t\t\t\t\t\t\tl = str(l).split('|')[0].replace('[[', '').replace(']]', '').replace(\"'\", \"\")                          \n",
    "\t\t\t\t\t\t\t\tif l in pages:                                 \n",
    "\t\t\t\t\t\t\t\t\tif '#REDIRECTION' in pages[l]:                                       \n",
    "\t\t\t\t\t\t\t\t\t\t#on suit la redirection\n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = relinks.search(pages[l]).group(1)                \n",
    "\t\t\t\t\t\t\t\t\t\tnewtitle = newtitle[0].upper()+newtitle[1:]\n",
    "\t\t\t\t\t\t\t\t\t\ts = newtitle\n",
    "\t\t\t\t\t\t\t\t\t\tif newtitle in pages:\n",
    "\t\t\t\t\t\t\t\t\t\t\tbib_source = pages[newtitle]\n",
    "\t\t\t\t\t\t\t\t\t\t\tif \"moyen breton\" in str(re.findall(r\"\\[\\[Category:.*\\|\", bib_source)):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ts += '(moyen breton)'                                           \n",
    "\t\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\t\ts = l                                              \n",
    "\t\t\t\t\t\t\t\t\t\tbib_source = pages[l]\n",
    "\t\t\t\t\t\t\t\t\tif re.findall(r\"\\[\\[Category:.*\\|\", bib_source):\n",
    "\t\t\t\t\t\t\t\t\t\tbibl = re.findall(r\"\\[\\[Category:.*\\|\", bib_source)                 \n",
    "\t\t\t\t\t\t\t\t\t\tfor ele in bibl:\n",
    "\t\t\t\t\t\t\t\t\t\t\ttdialect = reDialect.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif tdialect:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = tdialect.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\")\n",
    "\t\t\t\t\t\t\t\t\t\t\tttype = reType.search(ele)\n",
    "\t\t\t\t\t\t\t\t\t\t\tif ttype:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttexttype = ttype.group(0).capitalize().replace(\"[[Category:\", \"\").replace(\"|\", \"\") \n",
    "\t\t\t\t\t\t\t\t\t\tif not dialect: \n",
    "\t\t\t\t\t\t\t\t\t\t\tif reDialect.search(source):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdialect = reDialect.search(source).group(0)\n",
    "\t\t\t\t\t\t\t\tif reLanguage.search(source):\n",
    "\t\t\t\t\t\t\t\t\tgrammatical = False                                                 \n",
    "\t\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\t\ts = l\n",
    "\t\t\t\t\t\t\tif not location:\n",
    "\t\t\t\t\t\t\t\tif re.search(\"(''.*'')\", li):                                \n",
    "\t\t\t\t\t\t\t\t\ttest = re.search(\"''(.*)''\", li).group(1) \n",
    "\t\t\t\t\t\t\t\t\tif not reDialect.search(test) and not test.endswith(\"ais\"):                                     \n",
    "\t\t\t\t\t\t\t\t\t\tlocation = test \n",
    "                           \n",
    "\t\t\t# print(tokens, trans, translation, source)\n",
    "\t\t\tif not( tokens and trans and translation and source): # if one is missing\n",
    "\t\t\t\t#if verbose: print('Houston, we got a problem: The number of tokens and the number of transcriptions differ, or translation or source are missing:', title) \n",
    "\t\t\t\terrorout.write('\\n'+currentPage+': The number of tokens and the number of transcriptions differ or translation or source are missing:'+m.group(1)+'\\n')\n",
    "\t\t\t\tcontinue            \n",
    "\t#pour supprimer les mots agrammaticaux \n",
    "\t#exemple: ( C'hwi / * Ac'hanoc'h ) --> C'hwi  \n",
    "\t\tcl_tokens = []    \n",
    "\t\tfor i in range(len(tokens)-1):         \n",
    "\t\t\tif '[...]' in tokens[i]:\n",
    "\t\t\t\ttokens[i] = tokens[i].replace('[...]', '')\n",
    "\t\t\tif '(...)' in tokens[i]:\n",
    "\t\t\t\ttokens[i] = tokens[i].replace('(...)', '')              \n",
    "\t\t\tif re.match(r\".*\\((.*\\.)\\)\", title):            \n",
    "\t\t\t\tele = re.match(r\".*\\((.*)\\.\\)\", title).group(1)                \n",
    "\t\t\t\ttokens[i] = tokens[i].replace(ele, \"\")                \n",
    "\t\t\tif '/' in tokens[i] and '*' in tokens[i]:                \n",
    "\t\t\t\telems = tokens[i].split('/')                \n",
    "\t\t\t\tfor m in range(len(elems)):                    \n",
    "\t\t\t\t\tidx = m                    \n",
    "\t\t\t\t\tif '*' in elems[m]:                        \n",
    "\t\t\t\t\t\ttokens[i] = tokens[i].replace(elems[m], '').replace('(', '').replace('{', '').replace('/', '')   \n",
    "\t\t\t\t\t\tcl_tokens.append(tokens[i])                        \n",
    "\t\t\t\t\t\tfor i in range(len(trans)) :              \n",
    "\t\t\t\t\t\t\tif '/' in trans[i]:                       \n",
    "\t\t\t\t\t\t\t\ttrans[i] = trans[i].split('/')[idx].replace('(', '').replace('{', '').replace('/', '')\n",
    "\t\t\telif '/' in tokens[i] and not '*' in tokens[i]: \n",
    "\t\t\t\t#print(tokens)             \n",
    "\t\t\t\tif reAlternatives.search(tokens[i]):  \n",
    "\t\t\t\t\ttokens[i] = tokens[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')\n",
    "\t\t\t\t\ttrans[i] = trans[i].split('/')[0].replace('(', '').replace('{', '').replace('/', '')   \n",
    "\t\t\t\t\t#print(tokens[i])\n",
    "                                    \n",
    "\t\tif 'Équivalent standardisé' in tokens: \n",
    "\t\t\ttokens.remove('Équivalent standardisé')\n",
    "\t\t#supprime les éléments vides de la liste) \n",
    "  \n",
    "\t\t#print(f\"encore après {trans}\")              \n",
    "\t\ttokens = ' '.join(tokens).split()      \n",
    "\t\ttrans = ' '.join(trans).split()    \n",
    "\t\t#print(tokens, trans)\n",
    "\t\tif grammatical is False: \n",
    "\t\t\tbreak\n",
    "        \n",
    "\t\t# construction of the conll:  \n",
    "\t\ttext = ' '.join(tokens)\n",
    "\t\ttext_ch = remettretoken(text) \n",
    "\n",
    "\n",
    "\t\t# construction of the conll:         \n",
    "\t\ttext = ' '.join(tokens)\n",
    "\t\ttext_ch = remettretoken(text)        \n",
    "\t\t# print(text, text_ch)\n",
    "\t\treferences[source] = references.get(source,[])\n",
    "\t\tif text in references[source]:\n",
    "\t\t\tind = references[source].index(text)+1\n",
    "\t\telse:\n",
    "\t\t\treferences[source]+=[text]\n",
    "\t\t\tind = 1\n",
    "\t\tsentences[text] = sentences.get(text,0)+1\n",
    "\t\tconllis = ['# sent_id = '+source.replace(' ', '') +'__'+str(ind)]\n",
    "\t\tconllis += ['# text = '+text_ch]\n",
    "\t\t#MODIF !!! \n",
    "\t\tconllis += ['# text_fr = '+ remettretoken(cleanToken(translation)).replace('[...]', '').replace('(...)', '')]\n",
    "\t\tif phonetic:\n",
    "\t\t\tconllis += ['# text_phon = '+clean_phonetic.replace('[...]', '').replace('(...)', '')]\n",
    "\t\tif dialect:\n",
    "\t\t\tconllis += ['# dialect = ' + dialect]\n",
    "\t\tif location:\n",
    "\t\t\tconllis += ['# location = ' + location]\n",
    "\t\tif source2:\n",
    "\t\t\tconllis += ['# source2 = ' + source2]\n",
    "\t\tif s :\n",
    "\t\t\tconllis += ['# source = ' + s] \n",
    "\t\tif texttype :\n",
    "\t\t\tconllis += ['# texttype = ' + texttype]   \n",
    "\t\t##si on a trouvé du vert alors conllis += ['# phonetic = yes ']\n",
    "\t\t##si un morceau en phonétique alors mettre en trait MISC\n",
    "\n",
    "\t\textrai = 1\n",
    "\t\t# si un token en trop dans les tokens, alors enlever le dernier \n",
    "\t\t# case : ||| N'eo || ket || dereat || teañ || e || dad. || ''Équivalent standardisé''\n",
    "\t\t# if len(tokens) == len(trans)+1:\n",
    "\t\t# \ttokens = tokens[:-1] \n",
    "   \n",
    "\t\tfor i,t in enumerate(tokens):       \n",
    "\t\t\tif not t:\n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': strange token nr '+str(i)+' in ')\n",
    "\t\t\t\tt='???'\n",
    "\t\t\tif len(trans)<=i:\n",
    "\t\t\t\terrorout.write('\\n___ '+currentPage+': strange token nr '+str(i)+' in ')\n",
    "\t\t\t\ttr = '???'\n",
    "\t\t\telse:\n",
    "\t\t\t\ttr = trans[i]\n",
    "                \n",
    "\t\t\t# délimiteur : @ et espace \n",
    "\t\t\t# ATTENTION : tiret en délimiteur non traité pour l'instant !! (faire la même chose que la fonction sansespace, il faut splitter)\n",
    "\t\t\t# sur les tirets EN DEHORS des doubles crochets, non ceux qui sont à l'intérieur          \n",
    "\t\t\t#print(f\"t {t, tr}\")\n",
    "\t\t\t#print(tr)            \n",
    "\t\t\ttest1 = re.split(\"@| \", t)\n",
    "\t\t\tif '' in test1:  \n",
    "\t\t\t\ttest1.remove(\"\")         \n",
    "\t\t\ttest2 = sansespace(tr).split(\" \") \n",
    "\t\t\t#print(test1)\n",
    "\t\t\t# si on a le même nombre d'éléments pour les tokens et la glose entre chaque || ||\n",
    "\t\t\tif len(test1) == len(test2):               \n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]                  \n",
    "\t\t\t\t\ttr1 = test2[j]  \n",
    "#\t\t\t\t\tprint(f\"ici {t1, tr1}\")                    \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\tif t1[-1] not in \".,;!?\" and t1 != \"$$$\":  \n",
    "\t\t\t\t\t\t\tprint(\"1\")                \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)                         \n",
    "\t\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]# et morph ?\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")                    \n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "                        \n",
    "\t\t\t\t\t\tif t1[-1] in \".,;!?\" and not t1[-3:] == \"...\" :\n",
    "\t\t\t\t\t\t\tprint(\"2\") \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1[:-1], title, currentPage)\n",
    "\t\t\t\t\t\t\t#si on ne trouve pas de pos, on cherche dans la page elle-même\n",
    "\t\t\t\t\t\t\tif pos=='?' and t1[:-1].lower()==title.lower() and pagepos:\n",
    "\t\t\t\t\t\t\t\t# couldn't find a pos, maybe it's the word of the page itself?\n",
    "\t\t\t\t\t\t\t\tpos = pagepos\n",
    "\t\t\t\t\t\t\tdic[t1[:-1]] = dic.get(t1[:-1], [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[:-1]), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]          \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "\t\t\t\t\t\t\tdic[t1[-1:]] = dic.get(t1[-1:], [(t1[-1:], 'PUNCT','punct')])\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[-1:]), remettretoken(t1[-1:]), 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")  \n",
    "\t\t\t\t\t\t\t#print('\\t'.join(eles))\n",
    "\t\t\t\t\t\t\textrai += 1\n",
    "\n",
    "\t\t\t\t\t\tif t1[-3] == \"...\" :         \n",
    "\t\t\t\t\t\t\tprint(\"3\") \n",
    "\t\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1[:-3], title, currentPage)\n",
    "\t\t\t\t\t\t\t#si on ne trouve pas de pos, on cherche dans la page elle-même\n",
    "\t\t\t\t\t\t\tif pos=='?' and t1[:-3].lower()==title.lower() and pagepos:\n",
    "\t\t\t\t\t\t\t\t# couldn't find a pos, maybe it's the word of the page itself?\n",
    "\t\t\t\t\t\t\t\tpos = pagepos\n",
    "\t\t\t\t\t\t\tdic[t1[:-3]] = dic.get(t1[:-3], [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[:-3]), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]\n",
    "\t\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]                            \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\textrai += 3\n",
    "\t\t\t\t\t\t\tdic[t1[-3:]] = dic.get(t1[-3:], [(t1[-3:], 'PUNCT','punct')])\n",
    "\t\t\t\t\t\t\teles = [str(extrai),remettretoken(t1[-3:]), remettretoken(t1[-3:]), 'PUNCT']+4*['_']+['Gloss=punct']    \n",
    "\t\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")                             \n",
    "\t\t\t\t\t\t\textrai += 3 \n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tcontinue                        \n",
    "                        \n",
    "\t\t\t# si on a le nombre d'éléments de tokens est supérieur à celui de la glose                  \n",
    "\t\t\telif len(test1)>len(test2):\n",
    "\n",
    "\t\t\t\tfor j,e in enumerate(test1):\n",
    "\t\t\t\t\tt1 = test1[j]                       \n",
    "\t\t\t\t\tif t1 == \"$$$\":\n",
    "\t\t\t\t\t\tprint(\"AAAAAA\")                        \n",
    "\t\t\t\t\ttry:                    \n",
    "\t\t\t\t\t\ttr1 = test2[j]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\ttr1 = \"?\"\n",
    "\t\t\t\t\t# if t1.startswith(\"'\") and \"[[R\" in tr1.split(\"_\")[0] and reWord.search(tr1) : \n",
    "\t\t\t\t\t# \tm = re.match(r'(\\[\\[R\\]\\])(.*)', tr1)\n",
    "\t\t\t\t\t# \trannig = m.group(1)\n",
    "\t\t\t\t\t# \treste = m.group(2)                        \n",
    "\t\t\t\t\t# \ttr1 = [rannig, reste]\n",
    "\t\t\t\t\t# \tprint(tr1, type(tr1), type(tr1[0]))                        \n",
    "                        \n",
    "                        \n",
    "\t\t\t\t\t# si la ponctuation est tout simplement séparée du token                   \n",
    "\t\t\t\t\tif t1 in \"/-.,;!?\":\n",
    "\t\t\t\t\t\tprint(\"4\") \n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [(t1, 'PUNCT','punct')])\n",
    "\t\t\t\t\t\teles = [str(extrai),t1, t1, 'PUNCT']+4*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")                        \n",
    "\t\t\t\t\t\textrai += 1\n",
    "                        \n",
    "\t\t\t\t\tif t1 in \"...\":\n",
    "\t\t\t\t\t\tprint(\"5\") \n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [(t1, 'PUNCT','punct')])\n",
    "\t\t\t\t\t\teles = [str(extrai),t1, t1, 'PUNCT']+5*['_']+['Gloss=punct']\n",
    "\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")                  \n",
    "\t\t\t\t\t\textrai += 1             \n",
    "                    \n",
    "\t\t\t\t\t# les autres tokens sont traités normalement\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint(\"6\") \n",
    "\t\t\t\t\t\tlem, pos, gloss, morph = tokentrans2lemposglossmorph(tr1, t1, title, currentPage)\n",
    "\t\t\t\t\t\tdic[t1] = dic.get(t1, [])+[(lem,pos,gloss)]\n",
    "\t\t\t\t\t\teles = [str(extrai),remettretoken(t1), remettretoken(lem), pos]+5*['_']+['Gloss='+ remettretoken(gloss)]                       \n",
    "\t\t\t\t\t\tif morph != '':\n",
    "\t\t\t\t\t\t\ttry:                             \n",
    "\t\t\t\t\t\t\t\teles[9]+='|Mutation='+morph.split(\" \")[1]+'|MutationNUM='+morph.split(\" \")[0]\n",
    "\t\t\t\t\t\t\texcept: \n",
    "\t\t\t\t\t\t\t\tprint(\"error\") \n",
    "\t\t\t\t\t\tif len(eles) == 10:                              \n",
    "\t\t\t\t\t\t\tconllis += ['\\t'.join(eles)]\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tprint(f\"error: ligne du conll invalide (longueur {len(eles)})\")                            \n",
    "\t\t\t\t\t\textrai += 1\n",
    "\n",
    "\t\t\t# si on a le nombre d'éléments de la glose est supérieur à celui des tokens \n",
    "\t\t\telif len(test1)<len(test2):\n",
    "\t\t\t\tprint(\"PROBLEME ! éléments de la glose esupérieur à celui des tokens\")                \n",
    "\t\t\t\t#errorout.write('\\n___ '+currentPage+': strange token nr ' +str(i)+' in '+m.group(1) + 'test1' + str(len(test1)) + 'test2' + str(len(test2)))   \n",
    "\t\t#print(conllis)              \n",
    "\t\tif any(\"# dialect\" in string for string in conllis):         \n",
    "\t\t\tfor d in dialects:\n",
    "\t\t\t\tif f'# dialect = {d.capitalize()}' in conllis: \n",
    "\t\t\t\t\tdialects[d] += [conllis]  \n",
    "\t\telse: \n",
    "\t\t\tdialects['inconnu'] += [['\\n'.join(conllis)]]             \n",
    "            \n",
    "\tprint(nb_tables) \n",
    "\treturn dialects\n",
    "\n",
    "print('____________\\n')\n",
    "\n",
    "#for conll in analyzePage('Konduiñ'):\n",
    "analyzePage(\"Memes\")\n",
    "for d in dialects:\n",
    "\tfor conll in dialects[d]:   \n",
    "\t\tprint('\\n'.join(conll) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db823f65-8567-4f8f-ac74-de2d6895e0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11e098-4b26-46f1-8e26-43ec010492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b86d6f-2c73-46c4-8a22-824d7e9848dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for conll in analyzePage('Memes'):\n",
    "\tprint(conll,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6f25-c079-4dc0-9cee-e0654bf29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071bfc38-e641-4032-94d6-9510ceffcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDic():\n",
    "\tout = open('breton.dic.tsv','w', encoding=\"utf-8\")\n",
    "\tfor t, lpg in sorted(dic.items()):\n",
    "\t\tlpg = sorted(set(lpg))\n",
    "\t\t\n",
    "\t\tlines = ['\\t'.join([t,l,p,g]) for l,p,g in lpg]\n",
    "\t\tout.write('\\n'.join(lines)+'\\n')\n",
    "writeDic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb22511-47ca-492d-9e84-a842e73c638f",
   "metadata": {
    "id": "16b61a38"
   },
   "source": [
    "# trying to run through the whole site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d9e20-7981-4d54-9a76-f1df16eb2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                              | 26/8068 [00:00<00:32, 249.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                              | 51/8068 [00:00<00:39, 205.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                              | 88/8068 [00:00<00:29, 267.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                            | 152/8068 [00:00<00:27, 285.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                            | 182/8068 [00:00<00:28, 273.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██                                                                            | 210/8068 [00:00<00:31, 248.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                           | 264/8068 [00:00<00:23, 326.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                          | 355/8068 [00:01<00:15, 485.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                          | 406/8068 [00:01<00:24, 309.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                         | 478/8068 [00:01<00:39, 190.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                         | 505/8068 [00:02<00:51, 148.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n",
      "PROBLEME ! éléments de la glose esupérieur à celui des tokens\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "dic = {}\n",
    "errorout = open('breton.errors.txt','w', encoding=\"utf-8\")\n",
    "nopos = open('no_pos.txt', 'w', encoding='utf-8')\n",
    "for title in tqdm.tqdm(pages):\n",
    "\t# print('____________',title)\n",
    "\tanalyzePage(title)  \n",
    "for d in dialects:\n",
    "\tfor conll in dialects[d]:   \n",
    "\t\topen('bretonconlls/'+re.sub(r'\\W','_',d)+'.conllu','a', encoding=\"utf-8\").write('\\n'.join(conll) + '\\n\\n')    \n",
    "#\tif conlls:\n",
    "#\t\topen('bretonconlls/'+re.sub(r'\\W','_',title)+'.conllu','w', encoding=\"utf-8\").write('\\n\\n'.join(conlls))\n",
    "#writeDic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cbc9b-7747-42cf-90b0-834bfb7a4f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
